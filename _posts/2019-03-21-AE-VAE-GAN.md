---
layout: post
title: Algorithmic thinking and peak finding
date: 2019-03-21 19:32:24.000000000 +09:00
---

> 关于生成模型的一些小思考   https://zhuanlan.zhihu.com/p/25939348


VAE：

这里运用了 reparemerization 的技巧。由于 z∼N(μ,σ)z∼N(μ,σ)，我们应该从 N(μ,σ)N(μ,σ) 采样，但这个采样操作对 μμ 和 σσ 是不可导的，导致常规的通过误差反传的梯度下降法（GD）不能使用。通过 reparemerization，我们首先从 N(0,1)N(0,1) 上采样 ϵϵ，然后，z=σ⋅ϵ+μz=σ⋅ϵ+μ。这样，z∼N(μ,σ)z∼N(μ,σ)，而且，从 encoder 输出到 zz，只涉及线性操作，（ϵϵ 对神经网络而言只是常数），因此，可以正常使用 GD 进行优化。方法正确性证明见[1] 2.3小节和[2] 第3节 （stochastic backpropagation）。


