---
layout: post
title: unsupervised neutral machine translation with shared weight 
date: 2019-03-07 21:05:24.000000000 +09:00
---

本文出自2018ACL。

摘要

无监督神经机器翻译（NMT）是最近提出的用于机器翻译的方法，其旨在训练模型而不使用任何标记数据。 针对无监督NMT提出的模型通常仅使用一个共享编码器将来自不同语言的句子对映射到共享潜在空间，这在保持每种语言的独特和内部特征方面很弱，例如样式，术语和 句子的结构。 为了解决这个问题，我们通过利用两个独立的编码器引入扩展，但是共享一些部分权重，这些部分权重负责提取输入句子的高级表示。 此外，提出了两种不同的生成对抗网络（GAN），即本地GAN和全局GAN，以增强跨语言翻译。 通过这种新方法，我们在英语 - 德语，英语法语和汉语 - 英语翻译任务方面取得了显着的进步。

1.导言

神经机器翻译（Kalchbrenner和Blunsom，2013; Sutskever等人，2014; Cho等人，2014; Bahdanau等人，2014），直接应用单个神经网络将源句子转换为目标句子，现在达到了令人印象深刻的表现（Shen et al。，2015; Wu et al。，2016; Johnson et al。，2016; Gehring et al。，2017; Vaswani et al。，2017）。 NMT通常由两个子神经网络组成。编码器网络读取源语句并将其编码为上下文向量，并且解码器网络基于上下文向量迭代地生成目标语句。可以在监督和无监督学习环境中研究NMT。在监督环境中，双语语料库可用于训练NMT模型。在无人监督的环境中，我们只有两个独立的单语语料库，每种语言一个，并且没有双语训练的例子来提供两种语言的对齐信息。由于缺乏对齐信息，无监督的NMT被认为更具挑战性。然而，这项任务非常有希望，因为单语语料库通常很容易被收集。

最近在无监督的跨语言嵌入方面取得了成功（Artetxe等，2016; Zhang等，2017b; Conneau等，2017），无监督NMT提出的模型通常假设来自两种不同语言的一对句子可以映射到共享潜在空间中的相同潜在表示（Lample等，2017; Artetxe等，2017b）。按照这个假设，Lample等人（2017）对源语言和目标语言使用同一个编码器和同一个解码器。充当标准自动编码器（AE）的编码器和解码器被训练以重建输入。而Artetxe等人（2017b）使用共享编码器但使用两个独立的解码器。凭借一些良好的性能，它们共享一个明显的缺陷，即源语言和目标语言只共享一个编码器。虽然共享编码器对于将不同语言的句子映射到共享潜在空间至关重要，但它在保持每种语言的唯一性和内部特征方面很薄弱，例如风格，术语和句子结构。由于每种语言都有自己的特点，因此应该对源语言和目标语言进行独立编码和学习。因此，我们推测共享编码器可能是限制潜在翻译性能的因素。

为了解决这个问题，我们通过利用两个独立的编码器来扩展编码器共享模型，即具有一个共享编码器的模型，每个编码器用于一种语言。类似地，使用两个独立的解码器。对于每种语言，编码器及其对应的解码器执行AE，其中编码器从被扰动的输入句子生成潜在表示，并且解码器从潜在表示重建句子。为了将来自不同语言的潜在表示映射到共享潜在空间，我们建议对两个AE进行权重共享约束。具体来说，我们共享两个编码器的最后几层的权重，这两个编码器负责提取输入句子的高级表示。类似地，我们共享两个解码器的前几层的权重。为了强制执行共享潜在空间，单词嵌入在我们的编码器中用作增强编码组件。对于跨语言翻译，我们使用后面的反向翻译（Lample et al。，2017）。此外，还提出了两种不同的生成对抗网络（GAN）（Yang et al。，2017），即本地和全局GAN，以进一步改进跨语言翻译。我们利用局部GAN来约束源和目标潜在表示以具有相同的分布，由此编码器试图欺骗局部判别器，该局部判别器同时被训练以区分给定潜在表示的语言。我们应用全局GAN来微调相应的生成器，即另一种语言的编码器和解码器的组成，其中利用全局判别器来通过评估生成的句子与真实数据分布的距离来指导生成器的训练。总之，我们主要做出以下贡献：

我们向无监督的NMT提出权重共享约束，使模型能够为每种语言使用独立的编码器。 为了实施共享潜在空间，我们还为我们的模型提出了嵌入增强编码器和两个不同的GAN。

我们对英语 - 德语，英语 - 法语和汉语 - 英语翻译任务进行了大量实验。 实验结果表明，该方法始终取得了巨大成功。

最后但并非最不重要的是，我们引入了对所提出模型的模型时间顺序信息的directional自我attention。 实验结果表明，研究人员需要更多的努力来研究NMT自我attentional层内的时间顺序信息。

2.相关工作

已经提出了几种方法来训练没有直接平行语料库的NMT模型。 已被广泛研究的场景是两种语言之间几乎没有平行数据但通过一种枢轴语言很好地连接的场景。 这种情况下最典型的方法是从源语言到枢轴语言以及从枢轴语言到目标语言的独立翻译（Saha et al。，2016; Cheng et al。，2017）。 为了提高翻译绩效，约翰逊等人（2016）提出了标准NMT模型的多语言扩展，并且它们在没有直接平行训练数据的情况下实现了语言对的实质性改进。

最近，由于跨语言嵌入的成功，研究人员开始表现出对探索更加雄心勃勃的场景的兴趣，其中NMT模型仅受单语语料库的训练。 Lample等（2017）和Artetxe等（2017b）同时提出了这种情景的方法，该方法基于预先训练的跨语言嵌入。 Lample等（2017）对两种语言使用单个编码器和单个解码器。训练整个系统以重建其扰动输入。对于跨语言翻译，他们将反向翻译纳入训练程序。与（Lample等，2017）不同，Artetxe等（2017b）使用两个独立的解码器，每个解码器用于一种语言。上面提到的两个作品都使用单个共享编码器来保证共享的潜在空间。然而，伴随的缺陷是共享编码器在保持每种语言的唯一性方面很弱。我们的工作也属于这个更加雄心勃勃的场景，据我们所知，我们是首批研究如何仅使用单语语料库来训练NMT模型的人之一。

3.方法

3.1 模型架构

如图1所示，模型架构基于AE和GAN。它由七个子网络组成：包括两个编码器Encs和Enct，两个解码器Decs和Dect，本地判别器D1，以及全局判别器Dg1和Dg2。对于编码器和解码器，我们遵循新出现的transformer（Vaswani等，2017）。具体地说，编码器由四个相同的层组成。每层由多头自注意和简单的位置完全连接的前馈网络组成。解码器也由四个相同的层组成。除了每个编码器层中的两个子层之外，解码器还插入第三子层，其对编码器堆栈的输出执行多头注意。有关多头自我关注层的更多详细信息，请参阅读者（Vaswani等，2017）。我们将局部判别器实现为多层感知器，并实现基于卷积神经网络（CNN）的全局判别器。表1总结了解释子网络角色的几种方法。所提出的系统具有几个引人注目的组件，这些组件对于以无人监督的方式训练系统或改善翻译性能至关重要。

| 网络 | 作用 |
|:----:|:----:|
|{Encs, Decs}|源语言的AE|
|{Enct, Dect}|目标语言的AE|
|{Encs, Dect}|源语言到目标语言的翻译|
|{Enct, Decs}|目标语言到源语言的翻译|
|{Encs, Dl}|第一个局部GAN_l1|
|{Enct, Dl}|第2个局部GAN_l2|
|{Enct, Decs, Dg1}|第一个全局GAN_g1|
|{Encs, Dect, Dg2}|第二个全局GAN_g2|


