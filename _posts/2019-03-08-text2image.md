---
layout: post
title: Text2image
date: 2019-03-08 19:32:24.000000000 +09:00
---

今天翻译两篇text2image的论文。

> Text-to-image Synthesis via Symmetrical Distillation Networks

摘要

文本到图像合成旨在根据用户给出的文本描述自动生成图像，这是一项极具挑战性的任务。文本到图像合成的主要问题在于两个空白：异质和同质的差距。异构差距介于文本描述的高级概念和图像的像素级内容之间，而合成图像分布与实际图像分布之间存在同质差距。为了解决这些问题，我们利用通用判别模型（例如VGG19）的出色能力，可以在多个层面上指导新生成模型的训练过程，以弥合这两个空白。高级表示可以教导生成模型从文本描述中提取必要的视觉信息，这可以弥合异质性差距。中级和低级表示可以使其分别学习图像的结构和细节，从而减轻了同质性差距。因此，我们提出了对称蒸馏网络（SDN），其由作为“教师”的源判别模型和作为“学生”的目标生成模型组成。目标生成模型具有源对称模型的对称结构，以便可访问地传递分层知识。此外，我们将训练过程分解为两个阶段，使用不同的蒸馏范例来提升目标生成模型的性能。对两个广泛使用的数据集进行了实验，以验证我们提出的SDN的有效性。

1.导言

作为一种多媒体技术，跨模态检索[6,20,29,38,43]已成为大数据时代研究的一个突出主题。但是，跨模式检索有时无法满足用户的需求，因为它只能为用户提供现有数据。因此，跨模态合成的研究越来越受到社会的关注，如文本到图像的合成。文本到图像合成可以自动从文本描述生成图像。这是一项充满希望但具有挑战性的任务，它有两个主要问题。一方面，文本描述中的高级概念与合成图像的像素级内容之间存在异质差距。异质gap对应于输入文本描述和生成的图像之间的语义相关性。另一方面，由于巨大的图像像素空间，在合成和实际图像分布之间存在同质的gap。同质的gap反映在合成图像的真实性和质量上，包括全局结构和局部细节。因此，文本到图像合成的目标是弥合这些差距并生成具有高真实性，质量和语义相关性的图像，这些图像以文本描述为条件。

为了解决上述问题，我们考虑了判别任务的一般范式。 存在一种判别任务的一般范例，即首先通过标记图像的大规模数据集训练通用特征表示模型，然后针对特定任务微调模型[41]。 我们可以在各种最先进的方法中找到这种范例的许多成功应用，例如对象检测[7]，语义分割[21]和细粒度图像分类

相比之下，生成任务没有通用模型，大多数基于深度神经网络的现有生成方法都是从头开始训练他们的模型。然而，通用判别模型具有很强的产生多层次表示的能力，这可以通过反转和理解它的研究来验证[5,23,45]。通过反转和理解，可以注意到这些多级表示包含从像素内容到语义概念的分层信息。因此，利用通用判别模型的能力，有助于弥合文本到图像合成的异构和同质间隙。通用判别模型通常基于图像分类任务，并且可以预测输入图像的语义标签。它可以将图像的像素级内容映射到高级概念，这是针对文本到图像合成的逆过程。因此，通用判别模型产生的高级表示可以作为生成模型的指导，教导它们从文本描述中提取必要的视觉信息并弥合异质性差距。此外，中层和低层表示可以引导生成模型分别学习图像的结构和细节，从而减轻均匀间隙问题。与高维图像像素空间相比，更容易在这些低维特征空间中找到合成图像的最佳表示。因此，通过通用判别模型训练文本到图像合成的生成模型具有显着的优点。

在本文中，我们提出了对称蒸馏网络（SDN），以充分利用通用判别模型的强大功能，用于文本到图像合成的生成模型。 “蒸馏”范式受到[14]的启发。 Hinton等人。 [14]从Bucilua等人的模型压缩思想提出了“蒸馏”概念。 [3]，它可以将在大型数据集上训练的大型模型压缩到更小的模型。 我们的工作进一步将这一理念扩展到通用判别模型和新的生成模型之间。

> 补充材料  知识蒸馏  https://www.leiphone.com/news/201804/upBV7Zpd1jnsB9lf.html

我们设计了SDN的对称结构，包括源判别模型和目标生成模型（以下分别称为“源模型”和“目标模型”）。 源模型是通用判别模型（例如VGG19 [34]），并且目标生成模型具有相同的层但是与源模型的数据流方向相反。 对称结构使得可以在多个相应级别上从源模型向目标模型提取知识。

此外，我们设计了两个不同阶段的蒸馏方式：Stage-I和Stage-II蒸馏。 通过Stage-I蒸馏，目标模型从源模型粗略地给出的表示中学习，其以文本描述为条件绘制几乎包含所有视觉信息的模糊图像。 通过Stage-II蒸馏，目标模型更多地了解合成图像和真实图像之间在多个层次上的微小差异，最终合成具有关于对象的更多细节的图像。

我们在两个广泛使用的数据集上使用最先进的方法进行比较实验，以验证我们提出的SDN的有效性。 此外，进行了几个基线实验来分析不同组分的重要性。

2.相关工作

由于深度网络[19]，近年来图像生成取得了巨大进步。尽管像[25]这样的早期工作只能产生易于与实际样本区分的合成图像，但最近的工作[4,26,42]可以合成照片般逼真的图像。有一些典型的深度图像生成方法促进了这一进步。金马等人。通过使用概率图形模型并最大化数据可能性的下限来提出变分自动编码器（VAE）[17]以制定生成问题。 Goodfellow等人。提出生成性对抗网络（GAN）[8]在对抗范式中训练具有判别模型的生成模型。 Deep Recurrent Attention Writer（DRAW）方法[9]可以使用循环变分自动编码器和注意机制生成逼真的图像。作为一种自回归方法，PixelRNN [28]通过对像素空间中的条件分布建模来实现图像合成。此外，已经证明可以实现基于这些生成方法的条件图像生成，其具有更灵活的应用[36,40]。

在过去几年中，越来越多的研究人员关注文本到图像合成问题，这是一个以文本描述为条件的图像生成任务。由于文本图像合成的主要问题在于同质和异质gap，现有方法已经考虑了这些问题并试图对其进行处理。早期的工作更侧重于异质性gap。 Mansimov等[24]引入了AlignDRAW模型，通过软注意机制估计生成的图像和文本描述之间的对齐。但是，注意机制忽略了文本描述的一些视觉详细信息。里德等人[31]结合深对称结构化关节嵌入[30]和GAN [8]。嵌入方法可以挖掘生成图像的更详细的视觉信息，但由于基本的异构相关约束而不能充分利用这些信息。为了弥合均质的差距，Reed等人[32]建议GAWWN同时按对象和部分关键点位置约束控制全局结构。该方法可以成功生成具有正确对象形状和颜色的图像，但仍然缺乏细节的真实性。为了减轻均质gap问题，StackGAN [42]将文本到图像合成任务分为两个阶段。 StackGAN通过第一阶段的粗糙结构生成图像，并使用这些图像在第二阶段进一步生成具有更多局部细节的图像。受第一阶段的限制，最终生成的图像的结构具有一些不切实际的特征，并且仍存在均质的差距。

我们提出的SDN的生成模型不是在以前的方法中广泛使用的对抗性训练，而是具有前馈结构，并在通用判别模型的指导下学习。 有了这个范例，SDN不需要极小极大优化和两个对抗模型的仔细参数调整，这比当前基于GAN的方法更稳定[1]。

3.对称蒸馏网络

如图2所示，我们的SDN由源判别模型和目标生成模型组成，具有对称结构。 在训练阶段，有两个蒸馏阶段，通过多层次表示将知识从源模型转移到目标模型。 在测试阶段，SDN最终可以合成具有对象多级特征的图像。 此外，由于给定的文本描述可以对应于许多图像，因此我们将SDN扩展到不同的范例。

3.1 初步

3.5 执行细节

4.3 定性评估

对于定性评估，从测试集中随机选择一些文本描述。 对于每种比较方法，我们生成16个图像，每个文本描述都有条件，并选择最佳的图像进行比较。 我们的SDN还根据相同的文本描述合成相应的图像。

比较CUB-200-2011数据集的结果可以在图3中看到.GAN-INT-CLS生成的图像具有以文本描述为条件的正确全局结构，但在大多数情况下无法表达局部细节，例如生动的部分。尽管GAN-INT-CLS使用深对称结构化联合嵌入来减轻异质间隙，但由于简单的异构相关约束，其合成图像在细节上丢失了许多必要的视觉信息。 GAWWN通过使用对象和部分关键点位置约束以及其他条件变量来改善这种情况。但是，为了公平比较，我们将GAWWN限制为仅从文本描述生成图像而无需额外的位置注释，这与其他方法相同。 GAWWN生成的图像仅以文本描述为条件，但仍然缺乏细节的真实性。与上述方法相比，StackGAN可以使用来自文本描述的更多视觉信息生成更逼真的图像。然而，受StackGAN第一训练阶段的限制，最终合成图像的结构具有一些不切实际的特征。

我们的SDN解决了上述问题，并通过正确的全局结构，局部细节和文本描述中的必要视觉信息来合成图像，这些信息桥接了同质和异构间隙。 在图4中Oxford-Flower-102数据集的比较结果中可以看到相同的趋势。

4.6 近似样本检索

为了验证我们的SDN已经学习了真实图像的分布和异构相关而不是模仿训练样本的外观，我们进行了这种近似的样本检索。我们在CUB-200-2011或Oxford-Flower-102训练集上微调VGG19模型，并使用它来为SDN生成的图像和训练集中的所有图像提取fc7层视觉特征。然后我们检索训练集中具有合成图像的余弦相似性的所有图像，并选择前5个样本作为要呈现的最近邻居。在图7中展示了CUB-200-2011和Oxford-Flower-102训练集上的合成图像的前5个检索结果。显然，合成图像和检索到的训练样本之间的关键特征存在一定的差异。 ，虽然它们的颜色或形状相似。例如，在图7中的第三和第四行，即使检索到的花卉图像具有与合成图像查询相似的颜色，它们的形状也明显不同。换句话说，SDN根据文本描述创建其独特的花朵，这在训练集中是看不到的。

4.7 句子插值

我们通过句子插值实验呈现学习的潜在流形。我们使用线性插值句子嵌入来生成相应的图像，并在图8中显示四个示例。由于插入点没有正确的ground-truth文本描述，因此SDN生成的插值图像在细节上缺乏真实性。但是，SDN生成的图像的一些视觉特性也可以被区分，例如图8的所有四行中的渐变颜色。这是合成图像反映句子的含义变化的验证。例如，第四行中花的基本颜色从黄色变为红色。此外，在本地细节中可以找到相同的趋势。在第三行中，第一个左花的花瓣形状与第一个右花不同。正如我们所看到的，第二个右侧图像上的花瓣形状对于这两种花是独一无二的，这些花作为插入点而受到损害。我们可以得出结论，我们的SDN不仅在全局结构中而且在局部细节中学习平滑的异构流形，这是由于通用判别模型的分层知识蒸馏。

4.8 失败案例

我们在图9中的两个数据集上给出了一些失败案例。如图所示，这些失败案例主要是由Stage-I损失方案产生的不确定的对象形状引起的。 尽管这些失效情况的主要颜色基本正确，但由于阶段I蒸馏的限制，它们的物体形状是错误的。 源判别模型的知识是如此通用，以至于目标生成模型无法很好地学习某些文本描述的具体表示。 我们将在以后的工作中考虑并处理这个问题。


> 评注： 这篇北大的论文读下来感觉有些水啊


> stackGan: text to photo-realistic image synthesis with stacked generative adversarial networks

摘要

从文本描述合成照片般逼真的图像是计算机视觉中的挑战性问题，并且具有许多实际应用。现有的文本到图像方法生成的样本可以粗略地反映给定描述的含义，但它们不能包含必要的细节和生动的对象部分。在本文中，我们提出堆叠的生成对抗网络（StackGAN）来生成以文本描述为条件的照片真实图像。 Stage-I GAN基于给定的文本描述绘制对象的原始形状和基本颜色，产生Stage-I低分辨率图像。 Stage-II GAN将Stage-I结果和文本描述作为输入，并生成具有照片级真实细节的高分辨率图像。 Stage-II GAN能够纠正缺陷并通过细化过程添加引人注目的细节。 StackGAN生成的样本比现有方法生成的样本更合理。重要的是，我们的StackGAN首次生成仅以文本描述为条件的逼真256×256图像，而最先进的方法最多可生成128×128图像。为了证明所提出的StackGAN的有效性，对CUB和Oxford-102数据集进行了大量实验，这些数据集包含足够的对象外观变化并广泛用于文本到图像生成分析。

1.导言

从文本描述生成照片般逼真的图像是一个具有挑战性的问题。 当全自动合成系统可用时，它具有巨大的应用，包括照片编辑和计算机辅助设计。 然而，即使是最先进的方法也无法使用文本描述生成具有照片般逼真细节的高分辨率图像。 这个问题的主要挑战是给出文本描述的合理图像的空间是多模态的————有大量图像正确适合给定的文本描述。

最近，Generative Adversarial Networks（GAN）[7,3,19]在复杂多模态数据建模和合成真实世界图像方面取得了很好的成果。 里德等人。 [22,20]证明了GAN可以有效地生成以文本描述为条件的图像。 他们成功地以文本描述为条件生成了合理的64×64图像[22]。 然而，在许多情况下，它们的合成图像缺少细节和生动的物体部分，例如鸟的喙和眼睛。 而且，不提供对象的附加空间注释时它们不能合成更高分辨率的图像（例如，128×128）

为了应对这些挑战，我们将文本合成照片般逼真的图像合成问题分解为两个更易于管理的子问题，通过使用堆叠的生成对抗网络（StackGAN）。使用我们的Stage-I GAN生成低分辨率图像（图1（a））。该GAN学习绘制以给定文本描述为条件的生成对象的粗略形状和基本颜色，并从先验分布中采样的随机噪声向量生成背景区域。生成的低分辨率图像通常是粗糙的并且具有许多缺陷，例如，物体形状失真和没有物体部分。它可能看起来不真实，因为可能缺少一些令人信服的细节。在Stage-I GAN之上，我们堆叠Stage-II GAN以生成以低分辨率图像和相应文本描述为条件的逼真高分辨率图像（图1（b））。由于Stage-I GAN为对象和背景生成粗略的形状和布局，Stage-II GAN只需要关注绘制细节并纠正低分辨率图像中的缺陷。此任务比从头开始直接绘制高分辨率图像容易得多。通过再次调整文本，Stage-II GAN学习捕获Stage-I GAN省略的文本信息并绘制对象的更多细节。

本文的主要贡献是Stacked Generative Adversarial Networks（StackGAN）的设计，它可以从文本描述中合成照片般逼真的图像。 与现有的文本到图像生成模型相比，我们的StackGAN生成的图像具有更逼真的细节，并且在CUB [32]和Oxford102 [18]数据集的初始得分方面分别实现了28.47％和20.30％的改进。 我们提出的StackGAN可以生成高分辨率256×256图像，而最先进的方法在没有额外的标注信息时难以生成大于64×64的图像。

2.相关工作

生成图像建模是计算机视觉中的基本问题。最近，随着深度学习技术的出现，在这方面取得了显着进展。 Dosovitskiy等。 [5]训练反卷积神经网络生成3D椅子，桌子和汽车。其他小组[36,24]提出确定性神经网络作为图像合成的函数逼近器。与这些确定性方法相比，变分自动编码器（VAE）[12,25]用概率图形模型来解决问题，其目标是最大化数据可能性的下限。格雷戈尔等人。 [8]提出了DRAW模型，它将复现变分自动编码器与注意机制结合起来，为门牌号生成逼真的图像。利用神经网络来模拟像素空间的条件分布的自回归模型（例如，PixelRNN）[30]也产生了吸引人的合成图像。最近，Generative Adversarial Networks（GAN）[7]已经展示了用于生成更清晰样本的有希望的性能。鉴于GAN的训练动态不稳定，已经提出了几种技术[19,26]来稳定训练过程并在合成面部，室内和CIFAR-10图像上产生令人信服的结果。基于能量的GAN [37]也被提出用于更稳定的训练行为。

基于这些生成模型，还研究了条件图像生成。大多数方法使用简单的条件变量，如属性或类标签[35,31,2]。还有以图像为基础的工作来生成图像，例如照片编辑[1,38]，域转移[29]和超分辨率[27,14]。然而，超分辨率方法[27,14]只能为低分辨率图像添加有限的细节，并且不能像我们提出的StackGAN那样纠正大的缺陷。最近，已经开发了几种方法来从非结构化文本生成图像。 Mansimov等。 [16]通过学习估计文本和生成画布之间的对齐来构建AlignDRAW模型。里德等人。 [23]使用条件PixelCNN使用文本描述和对象位置约束生成图像。使用条件GAN，Reed等。 [22]从文字描述中成功地为鸟类和花朵生成了合理的64×64图像。他们的后续工作[20]能够通过使用附加注释控制对象位置来生成更高分辨率的图像，例如对象的边界框或部分关键点。

除了使用单个GAN生成图像之外，还有使用一系列GAN进行图像生成的工作[33,3]。王等人。 [33]利用提出的S 2 -GAN将室内场景生成过程分解为结构生成和样式生成。 Style-GAN的输入图像是从Structure-GAN中提取的样本。两个GAN专注于生成两种不同模态的图像。相比之下，StackGAN的第二阶段旨在完成对象细节并根据文本描述纠正Stage-I结果的缺陷。 Denton等人。 [3]在拉普拉斯金字塔框架内构建了一系列GAN。在金字塔的每个级别，以前一级的图像为条件生成残差图像，然后将其添加回输入图像以产生下一级的输入。然而，他们只成功生成96×96图像，而我们的方法利用更简单的架构生成256×256图像，具有逼真的细节和六倍的像素。

3.stacked GAN

3.1 预备知识

条件GAN [6,17]是GAN的扩展，其中生成器和鉴别器都接收附加的条件变量c，产生G（z，c）和D（x，c）。 该公式允许G生成以变量c为条件的图像。

"StackGAN 没有直接将 embedding 作为 condition ，而是用 embedding 接了一个 FC 层从得到的独立的高斯分布中随机采样得到隐含变量。之所以这样做的原因是，embedding 通常比较高维，而相对这个维度来说， text 的数量其实很少，如果将 embedding 直接作为 condition，那么这个 latent variable 在 latent space 里就比较稀疏，这对训练不利。"

本文也是一种条件GAN，应满足的条件是c,即文本的嵌入向量，但本文又将其增强了一下。    

> CGAN： https://blog.csdn.net/Solomon1558/article/details/52555083

3.3 stage-2 GAN

最后一段：

对于鉴别器，其结构类似于Stage-I鉴别器的结构，仅具有额外的下采样块，因为在该阶段图像尺寸较大。 为了明确地强制GAN学习图像和条件文本之间更好的对齐，而不是使用天真的鉴别器，我们采用了Reed等人提出的匹配感知鉴别器。 [22]两个阶段。 在训练期间，鉴别器将实际图像及其相应的文本描述作为正样本对，而负样本对由两组组成。 第一种是具有不匹配的文本嵌入的真实图像，而第二种是具有条件文本嵌入的合成图像。

> 稍等
