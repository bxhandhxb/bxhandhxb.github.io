---
layout: post
title: PGM notes4 
date: 2019-02-08 21:25:24.000000000 +09:00
---

今天要学习的是cmu的课程lec4。

> Partially Directed Acyclic Graphs(chain graphs): superset of both BNs and MNs

用的比较少

Why we care about this:

BN and MN offer different semantics for designer to capture or expression
(conditional) independences among variables

Under certain condition BN can be represented as an MN and vice versa

In the future, for certain operation (i.e., inference), we will be using a single representation as the “data structure” for which an algorithm can operate on.

This makes algorithm design, and analysis of the algorithms simpler

PGM减少cost of representation 和 cost of computation，它是设计高效学习和推断算法的工具。

> Probabilistic Inference and Learning

We now have compact representations of probability distributions: Graphical Models

A GM M describes a unique probability distribution P 

Typical tasks:

Task 1: How do we answer queries about PM ?

We use inference as a name for the process of computing answers to such queries

Task 2: How do we estimate a plausible model M from data D?

i. We use learning as a name for the process of obtaining point estimate of M.

ii. But for Bayesian, they seek p(M/D), which is actually an inference problem.

iii. When not all variables are observable, even computing point estimate of M need to do inference to impute the missing data.

//学习其实是对参数的推断

> Task 1: inference 

1.likelihood    

P(e) = 

2.Conditional Probability

P(X/e) :  this is the a posteriori belief in X, given evidence e

应用：

Prediction: what is the probability of an outcome given the starting
condition(the query node is a descendent of the evidence)

方法：P(C/A,B)=P(C/B)

Diagnosis: what is the probability of disease/fault given symptoms(he query node an ancestor of the evidence)

方法：P(A/B,C)=P(A/B)

Learning under partial observation

方法：fill in the unobserved values under an "EM" setting (more later)

注意： 推断的顺序没有被gm中的方向所限制，可以由尾推头。

Example: Deep Belief Network(DBN: 深度信念网络)

3.Most Probable Assignment

this is the maximum a posteriori configuration of y.

argmax(y){P(y/e)}

注意： MPA of Y1  may be unequal to the MPA of Y1、Y2. 换句话说，单个变量y的取值要看query中其他的变量，是取query中所有变量的联合概率最大的值对应的y。

> Computing P(X = x / e) in a GM is NP-hard

某些gm可以多项式复杂度的精确计算，而一些则不能，需要近似计算。


方法roadmap：

1.Exact inference algorithms

i.The elimination algorithm

ii.Message-passing algorithm (sum-product, belief propagation)

iii.The junction tree algorithms

2.Approximate inference techniques

i.Stochastic simulation / sampling methods

ii.Markov chain Monte Carlo methods

iii.Variational algorithms


> The elimination algorithm: 

motivation: elimination in chains

Complexity:

Each step costs O(/Val(Xi)/*/Val(Xi+1)/) operations: O(kn^2)

Compare to naïve evaluation that sums over joint values of n-1 variables O(n^k)

generic: HMM













