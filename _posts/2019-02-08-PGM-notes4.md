---
layout: post
title: PGM notes4 
date: 2019-02-08 18:25:24.000000000 +09:00
---

今天要学习的是cmu的课程lec4。

> Partially Directed Acyclic Graphs(chain graphs): superset of both BNs and MNs

用的比较少

Why we care about this:

BN and MN offer different semantics for designer to capture or expression
(conditional) independences among variables

Under certain condition BN can be represented as an MN and vice versa

In the future, for certain operation (i.e., inference), we will be using a single representation as the “data structure” for which an algorithm can operate on.

This makes algorithm design, and analysis of the algorithms simpler

PGM减少cost of representation 和 cost of computation，它是设计高效学习和推断算法的工具。

> Probabilistic Inference and Learning

We now have compact representations of probability distributions: Graphical Models

A GM M describes a unique probability distribution P 

Typical tasks:

Task 1: How do we answer queries about PM ?

We use inference as a name for the process of computing answers to such queries

Task 2: How do we estimate a plausible model M from data D?

i. We use learning as a name for the process of obtaining point estimate of M.

ii. But for Bayesian, they seek p(M/D), which is actually an inference problem.

iii. When not all variables are observable, even computing point estimate of M need to do inference to impute the missing data.

//学习其实是对参数的推断

> Task 1: inference 

1.likelihood    

P(e) = 

2.Conditional Probability

P(X/e) :  this is the a posteriori belief in X, given evidence e

应用：

Prediction: what is the probability of an outcome given the starting
condition(the query node is a descendent of the evidence)

方法：P(C/A,B)=P(C/B)

Diagnosis: what is the probability of disease/fault given symptoms(he query node an ancestor of the evidence)

方法：P(A/B,C)=P(A/B)

Learning under partial observation

方法：fill in the unobserved values under an "EM" setting (more later)


3.







